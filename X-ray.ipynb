{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set: \n",
    "Link to the data set https://stanfordmlgroup.github.io/competitions/mura/\n",
    "MURA consists of images 7 types: hand, humerus, forearm, finger, wrist, elbow, shoulder. The purpose is to build a model that classifies images as negative ('healthy' - 0) or positive('abnormal' - 1). There is a 'train' set and a 'valid' set (used actually for testing) inside the archive. Note that the total data is very large (approx 3 GB)\n",
    "\n",
    "Data extraction:\n",
    "Using a separate scipt I've split images based on their kind. In folders 'train_specific_paths' and 'valid_specific_paths' one can find .csv files which contain the necessary paths used to extract data.\n",
    "Here, use the functions extract_data (to extract specific kind of images; ex: 'elbow') and extract_all_data. These functions return a 4-tuple: np arrays representation of train images, labels of train images, test images, labels of test images.\n",
    "Note that when calling extract_data or extract_all_data, the first parameter is the path to the location of MURA (so if needed, please change 'D:\\\\python' in order to make the example work)\n",
    "\n",
    "Model:\n",
    "\n",
    "Given the task is image classification, convolutional neural network (CNN) is the most suitable type of model. Below one can find my reasoning for the design decisions made.\n",
    "\n",
    "Preprocessing: MURA images are either RGB or greyscale. Because of the nature of classification, converting them to greyscale is preferable. Images have different shapes, I chose to reshape all of them to (512,512), because 512 is the maximum value their shaes can take and so no information will be lost.\n",
    "\n",
    "Choosing the right parameters: using k_fold_cross_validation (k = 5) and classic_validation functions I computed the values to be used for the 2 pairs of convoltuion layers + maxpooling and the size of the fully connected dense layer. 3 different types of models have been trained, the one selected was the one with better accuracy on the test set.\n",
    "Accuracy of the presented model: 61% on test set \n",
    "\n",
    "Metrics used: accuracy and confusion matrix (see conf_matrix function)\n",
    "Dealing with overfitting: because of the large size of the train set, overfitting can be an issue. I've added dropout layers (of small values for the conv layers: 0.2 and 0.5 for the dense layer) and mainted a validation set (in classic_validation 20% of train data is saved for validation)\n",
    "\n",
    "Limitations:The large size of the data set, makes training models take a very long time ( approx. 5 hours to train a model on elbow images alone) and so more experimenting is required in order to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os \n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from csv import reader\n",
    "from sklearn.preprocessing import scale\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling2D, Flatten, Dense, Dropout \n",
    "from tensorflow.keras import models\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(source, kind, reshape, scale):\n",
    "    train_imgs, train_vals = extract_helper(source, 'train', kind, reshape, scale)\n",
    "    test_imgs, test_vals = extract_helper(source, 'valid', kind, reshape, scale)\n",
    "    return train_imgs, train_vals, test_imgs, test_vals\n",
    "\n",
    "def extract_all_data(source, reshape, scale):\n",
    "    train_imgs, train_vals = extract_helper(source, 'train', reshape, scale)\n",
    "    test_imgs, test_vals = extract_helper(source, 'valid', reshape, scale)\n",
    "    return train_imgs, train_vals, test_imgs, test_vals\n",
    "\n",
    "\n",
    "def extract_helper(source, torv, kind, reshape, scale):\n",
    "    os.chdir(source+'\\\\MURA-v1.1')\n",
    "    os.chdir(torv+'_specific_paths')\n",
    "    file = open(torv+'_image_paths_'+kind+'.csv')\n",
    "    return extract(source, file, reshape, scale)\n",
    "\n",
    "def extract_all_helper(source, torv, reshape, scale):\n",
    "    os.chdir(source+'\\\\MURA-v1.1')\n",
    "    file = open(torv+'_image_paths_.csv')\n",
    "    return extract(source, file, reshape, scale)\n",
    "    \n",
    "def extract(source, file, reshape, scale):\n",
    "    readCSV = reader(file)\n",
    "    imgs = []\n",
    "    vals = []\n",
    "    for row in readCSV:\n",
    "        #im = Image.open(source+'\\\\'+row[0]).convert('L').resize(reshape)\n",
    "        #train_imgs.append(scale(np.array(im)))\n",
    "        im = cv2.imread(source+'\\\\'+row[0], cv2.IMREAD_GRAYSCALE)\n",
    "        if scale == True:\n",
    "            imgs.append(scale(np.array(cv2.resize(im,reshape))))\n",
    "        else:\n",
    "            imgs.append(np.array(cv2.resize(im,reshape)))\n",
    "        if 'positive' in row[0]:\n",
    "            vals.append(1)\n",
    "        else:\n",
    "            vals.append(0)\n",
    "    file.close()\n",
    "    imgs = np.array(imgs)\n",
    "    vals = np.array(vals)\n",
    "    imgs = np.expand_dims(imgs, axis=3)\n",
    "    return imgs,vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4931, 512, 512, 1)\n",
      "(465, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "# *** Example ***\n",
    "e_train_x, e_train_y, e_test_x, e_test_y = extract_data('D:\\\\python','elbow', (512,512), False)\n",
    "print(e_train_x.shape)\n",
    "print(e_test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_validation(model, data_x, data_y, batch_size, number_of_epochs):\n",
    "    l = len(data_y)\n",
    "    rate = int(l*0.8)\n",
    "    p = np.random.permutation(l)\n",
    "    data_x = data_x[p]\n",
    "    data_y = data_y[p]\n",
    "    train_x = data_x[:rate]\n",
    "    train_y = data_y[:rate]\n",
    "    valid_x = data_x[rate:]\n",
    "    valid_y = data_y[rate:]    \n",
    "    model.fit(train_x, train_y, batch_size = batch_size, epochs = number_of_epochs)\n",
    "    score = model.evaluate(valid_x,valid_y)[1]\n",
    "    model.fit(valid_x, valid_y, batch_size = batch_size, epochs = number_of_epochs)\n",
    "    return score, model\n",
    "\n",
    "def k_fold_cross_validation(k, model, data_x, data_y, batch_size, number_of_epochs):\n",
    "    l = len(data_y)\n",
    "    p = np.random.permutation(l)\n",
    "    data_x = data_x[p]\n",
    "    data_y = data_y[p]\n",
    "    folds_x = []\n",
    "    folds_y = []\n",
    "    for i in range(k):\n",
    "        folds_x[i] = data_x[(l//k)*i: (l//k)*(i+1)]\n",
    "        folds_y[i] = data_y[(l//k)*i: (l//k)*(i+1)]\n",
    "    score = 0\n",
    "    for i in range(k):\n",
    "        model_copy = models.clone_model(model)\n",
    "        for j in range(k):\n",
    "            if j!=i:\n",
    "                model_copy.fit(folds_x[j],folds_y[j], batch_size = batch_size, epochs = number_of_epochs)\n",
    "        score += model_copy.evaluate(folds_x[i],folds_y[i])[1]\n",
    "    model = mode_copy\n",
    "    model.fit(folds_x[k-1],folds_y[k-1], batch_size = batch_size, epochs = number_of_epochs)\n",
    "    return score/k, model\n",
    "\n",
    "def conf_matrix(model, data_x, data_y):\n",
    "    y_pred = model.predict(data_x).flatten().tolist()\n",
    "    y_true = data_y.tolist()\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = round(y_pred[i])\n",
    "    return confusion_matrix(y_true, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-39dd41ed550a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassic_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_train_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_train_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me_test_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_test_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_test_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_test_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-57d77eab0046>\u001b[0m in \u001b[0;36mclassic_validation\u001b[1;34m(model, data_x, data_y, batch_size, number_of_epochs)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mrate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdata_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdata_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# *** Example ***\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Conv2D(8, (8, 8), activation='relu', input_shape=(512,512,1), padding = 'same'))\n",
    "model.add(Conv2D(8, (8, 8), activation='relu', padding = 'same'))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(16, (8, 8), activation='relu', padding = 'same'))\n",
    "model.add(Conv2D(16, (8, 8), activation='relu', padding = 'same'))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "s, model = classic_validation(model, e_train_x, e_train_y, 8, 5)\n",
    "model.evaluate(e_test_x, e_test_y)\n",
    "print(conf_matrix(model, e_test_x, e_test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
